{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_PATH = 'files_to_analyze'\n",
    "OUTPUT_PATH = 'output'\n",
    "\n",
    "import os\n",
    "import re\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getDatasetSessionDict (dataset_path):\n",
    "\n",
    "    \"\"\"Helper function to do create an output dict containing k,v pairs of session names and their corresponding CSV file locations.\n",
    "    \n",
    "    Args:\n",
    "        dataset_path (str): Directory name of the dataset/session to be processed (folder name in /files_to_analyze).\n",
    "        \n",
    "    Returns:\n",
    "        dict: keys = unique recording session names in the dataset, values = file locations of all CSVs taken in this recording session (e.g., same session filename).\n",
    "    \"\"\"\n",
    "    \n",
    "    csv_regex = re.compile(r'.*\\.csv$') #regex to match CSV files only.\n",
    "    csv_filenames = [item for item in os.listdir(dataset_path) if csv_regex.match(item)] # list of CSV filenames in dataset_path (ex.: \"AA64-0023.csv\").\n",
    "    csv_paths = [os.path.join(dataset_path, csv_filename) for csv_filename in csv_filenames] # list of CSV file locations (ex.: \"file/location/AA64-0023.csv\").\n",
    "    csv_names = [os.path.splitext(os.path.basename(location))[0] for location in csv_paths] # list of CSV file names w/o file type (ex.: \"AA64-0023\").\n",
    "    session_names = [name.split('-')[0] for name in csv_names] # list of session names for each CSV file (ex.: \"AA64\").\n",
    "    unique_session_names = list(set(session_names)) # list of unique session names\n",
    "\n",
    "    # Create a dictionary with session names as keys and lists of corresponding file locations as values.\n",
    "    dataset_session_dict = {session: [csv_paths[i] for i in range(len(csv_paths)) if session_names[i] == session] for session in unique_session_names}\n",
    "    \n",
    "    return dataset_session_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "def error_check (dir, error_dict):\n",
    "    \n",
    "    if error_dict['channel_error']:\n",
    "        print(f'>! Error: mis-match in declared and recorded channels detected in {dir}.')\n",
    "        print('\\tData from the following recordings were not saved to the session data file:')\n",
    "        for file in error_dict['unsaved_data_files']:\n",
    "            print(f'\\t\\t{file}')\n",
    "        pass\n",
    "\n",
    "    if error_dict['multipleSessions_error']:\n",
    "        print(f'>! Error: multipleSessions_error in {dir}.')\n",
    "        print('\\tData from the following recordings were not saved to the session data file:')\n",
    "        for file in error_dict['unsaved_data_files']:\n",
    "            print(f'\\t\\t{file}')\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pickle_session(dir, session_name, csv_paths, output_path):\n",
    "\n",
    "    \"\"\"Helper function to do create a pickle file from a single session dataset of CSV files.\n",
    "    \n",
    "    Args:\n",
    "        session_name (str): name of the session to be processed.\n",
    "        csv_paths (list): Relative file paths of all CSV files in the dataset.\n",
    "        output_path (str): Path to the /output folder or other location for Pickle files to be saved.\n",
    "        \n",
    "    Returns:\n",
    "        tuple: A tuple containing the total number of CSV files in the given session/dataset and the number of CSV files that were successfully included in the Pickle file.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Gather session_info from the first CSV of a dataset\n",
    "    first_csv = csv_paths[0]\n",
    "    with open(first_csv, 'r') as file:\n",
    "        lines = file.readlines() # load .csv data lines into memory\n",
    "\n",
    "        # load desired session parameters\n",
    "        session_id = float(next(line.split(',')[1] for line in lines if line.startswith('Session #,')))\n",
    "        num_channels = int(float(next(line.split(',')[1] for line in lines if line.startswith('# of Channels,'))))\n",
    "        scan_rate = float(next(line.split(',')[1] for line in lines if line.startswith('Scan Rate (Hz),')))\n",
    "        num_samples = float(next(line.split(',')[1] for line in lines if line.startswith('Samples/Channel,')))\n",
    "        \n",
    "        stim_duration = float(next(line.split(',')[1] for line in lines if line.startswith('Stimulus duration (ms),')))\n",
    "        stim_interval = float(next(line.split(',')[1] for line in lines if line.startswith('Inter-Stim delay (sec),')))\n",
    "        \n",
    "        emg_amp_gains = []\n",
    "        for line in lines:\n",
    "            if line.startswith('EMG amp gain ch'):\n",
    "                emg_amp_gains.append(int(float(line.split(',')[1])))\n",
    "\n",
    "    # Create a dictionary to store the session data\n",
    "    session_data = {\n",
    "        'session_info': {\n",
    "            'session_name' : session_name,\n",
    "            'num_channels': num_channels,\n",
    "            'scan_rate': int(scan_rate),\n",
    "            'num_samples': int(num_samples),\n",
    "            'stim_duration' : stim_duration,\n",
    "            'stim_interval' : stim_interval,\n",
    "            'emg_amp_gains': emg_amp_gains\n",
    "        },\n",
    "        'recordings': []\n",
    "    }\n",
    "    # Initialize a dictionary to store information about processing errors\n",
    "    error_dict = {\n",
    "        'channel_error' : False,\n",
    "        'multipleSessions_error': False,\n",
    "        'unsaved_data_files' : []\n",
    "    }\n",
    "\n",
    "    # Process each recording for stimulus and EMG data\n",
    "    for recording_file in csv_paths:  # Replace with your list of recording files\n",
    "        with open(recording_file, 'r') as file:\n",
    "            lines = file.readlines() # load CSV lines into memory.\n",
    "            test_session_id = float(next(line.split(',')[1] for line in lines if line.startswith('Session #,')))\n",
    "            \n",
    "            # Test if a second session's file is detected. This should never be true.\n",
    "            if test_session_id != session_id: \n",
    "                error_dict['unsaved_data_files'].append(recording_file) if recording_file not in error_dict['unsaved_data_files'] else None\n",
    "                error_dict['multipleSessions_error'] = True\n",
    "            \n",
    "        \n",
    "            # Extract Stimulus Value (in volts)\n",
    "            stimulus_v = float(next(line.split(',')[1] for line in lines if line.startswith('Stimulus Value (V),')))\n",
    "\n",
    "            # Extract EMG \"Recorded Data (mV)\"\n",
    "            start_index = None\n",
    "            data_lines = []\n",
    "            for i, line in enumerate(lines):\n",
    "                if line.startswith(\"Recorded Data (mV),\"):\n",
    "                    start_index = i + 1\n",
    "                elif start_index is not None:\n",
    "                    data_lines.extend([value.split(',') for value in line.strip().split('\\n')])\n",
    "                    if line.strip() == \"\":\n",
    "                        break\n",
    "            # test for a mis-match in the cvs number of channels and the declared number of channels.\n",
    "            if num_channels != len(data_lines[0]):\n",
    "                error_dict['unsaved_data_files'].append(recording_file) if recording_file not in error_dict['unsaved_data_files'] else None\n",
    "                error_dict['channel_error'] = True\n",
    "                continue\n",
    "            \n",
    "            # Create a list to store data for each channel\n",
    "            channel_data = [[] for _ in range(num_channels)]\n",
    "\n",
    "            # Populate the channel data\n",
    "            for row in data_lines:\n",
    "                for i, value in enumerate(row):\n",
    "                    channel_data[i].append(float(value))\n",
    "\n",
    "            # Update the session_data with the number of channels and channel_data\n",
    "            session_data['session_info']['num_channels'] = num_channels\n",
    "            session_data['recordings'].append({\n",
    "                'stimulus_v': stimulus_v,\n",
    "                'channel_data': channel_data\n",
    "            })\n",
    "                    \n",
    "    # Save the session data to its own pickle file only if there were not multiple sessions detected.\n",
    "    save_name = f'{dir}_{session_name}-SessionData.pickle'\n",
    "    with open(os.path.join(output_path, save_name), 'wb') as pickle_file:\n",
    "        pickle.dump(session_data, pickle_file)\n",
    "    \n",
    "    # Confirm Pickled session CSVs and return whether the given file is a multi-session dataset that needs to be handled differently.\n",
    "    error_check(dir, error_dict)\n",
    "    num_csvs = len(csv_paths)\n",
    "    num_csv_success = len(csv_paths) - len(error_dict['unsaved_data_files'])\n",
    "    print(f'> {num_csv_success} of {num_csvs} CSVs processed from dataset \"{session_name}\".')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Datasets to process: ['040224rec1', '240507 mouse 1 post-dec m ramp', '240507 mouse 1 pre-dec', 'hidden']\n",
      "> 34 of 34 CSVs processed from dataset \"AA64\".\n",
      "> 35 of 35 CSVs processed from dataset \"C046\".\n",
      "> 47 of 47 CSVs processed from dataset \"C050\".\n",
      "> 61 of 61 CSVs processed from dataset \"C047\".\n",
      "> 37 of 37 CSVs processed from dataset \"C051\".\n",
      "> 36 of 36 CSVs processed from dataset \"C049\".\n",
      "> 57 of 57 CSVs processed from dataset \"C048\".\n",
      "> 38 of 38 CSVs processed from dataset \"C045\".\n",
      "> 27 of 27 CSVs processed from dataset \"C024\".\n",
      "> 27 of 27 CSVs processed from dataset \"C025\".\n",
      "> 31 of 31 CSVs processed from dataset \"C023\".\n",
      "> 28 of 28 CSVs processed from dataset \"C022\".\n",
      "> 37 of 37 CSVs processed from dataset \"C021\".\n",
      ">! Error: no CSV files detected in \"hidden.\" Make sure you converted STMs to CSVs.\n",
      "Processing complete.\n"
     ]
    }
   ],
   "source": [
    "# def pickle_data (data_path, output_path):\n",
    "\n",
    "data_path = DATA_PATH\n",
    "output_path = OUTPUT_PATH\n",
    "\n",
    "# Process \"\\files_to_analyze\" into Pickle files\n",
    "datasets = [dir for dir in os.listdir(data_path) if os.path.isdir(os.path.join(data_path, dir))] # list all datasets in \"files_to_analyze\" folder\n",
    "print(f'Datasets to process: {datasets}')\n",
    "\n",
    "# Create a pickle file for each dataset in files_to_analyze\n",
    "for dir in datasets:\n",
    "    dataset_path = os.path.join(data_path, dir)#.replace('\\\\', '/')\n",
    "    dataset_session_dict = getDatasetSessionDict(dataset_path)\n",
    "    multipleSessions = False # Default this value to False for each directory until proven otherwise.\n",
    "    \n",
    "    # check if there are even CSVs in this dataset.\n",
    "    if len(dataset_session_dict) <= 0:\n",
    "        print(f'>! Error: no CSV files detected in \"{dir}.\" Make sure you converted STMs to CSVs.')\n",
    "        continue\n",
    "\n",
    "    # check if there are multiple sessions in the \n",
    "    if len(dataset_session_dict) > 1:\n",
    "        multipleSessions = True\n",
    "\n",
    "    # Process the dataset if it contains only a single session.\n",
    "    if not multipleSessions:\n",
    "        session_name, csv_paths = next(iter(dataset_session_dict.items()))\n",
    "        pickle_session(dir, session_name, csv_paths, output_path)\n",
    "        continue\n",
    "        # csv_paths = dataset_session_dict\n",
    "        # pickle_session(dir, csv_paths, output_path)\n",
    "\n",
    "    # Process the dataset if it contains multiple sessions and error check.\n",
    "    if multipleSessions:\n",
    "        dataset_output_path = os.path.join(output_path,dir)\n",
    "        if not os.path.exists(dataset_output_path):\n",
    "            os.mkdir(dataset_output_path)\n",
    "\n",
    "        for session_name, csv_paths in dataset_session_dict.items():\n",
    "            pickle_session(dir, session_name, csv_paths, dataset_output_path)\n",
    "\n",
    "print('Processing complete.')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "open_ephys",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
