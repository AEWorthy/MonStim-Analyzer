{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_PATH = 'files_to_analyze'\n",
    "OUTPUT_PATH = 'output'\n",
    "\n",
    "import os\n",
    "import re\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pickle_dataset(dataset, csv_paths, output_path):\n",
    "\n",
    "    \"\"\"Helper function to do create a pickle file from a dataset of session CSV files.\n",
    "    \n",
    "    Args:\n",
    "        dataset (str): Directory name of the dataset/session to be processed (folder name in /files_to_analyze).\n",
    "        csv_paths (list): Relative file paths of all CSV files in the dataset.\n",
    "        output_path (str): Path to the /output folder or other location for Pickle files to be saved.\n",
    "        \n",
    "    Returns:\n",
    "        tuple: A tuple containing the total number of CSV files in the given session/dataset and the number of CSV files that were successfully included in the Pickle file.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Gather session_info from the first CSV of a dataset\n",
    "    first_csv = csv_paths[0]\n",
    "    session_id = None\n",
    "    channel_error = False # flag for test of number of channels\n",
    "   \n",
    "    with open(csv, 'r') as file:\n",
    "        lines = file.readlines() # load .csv data lines into memory\n",
    "\n",
    "        # load desired session parameters\n",
    "        session_id = float(next(line.split(',')[1] for line in lines if line.startswith('Session #,')))\n",
    "        num_channels = int(float(next(line.split(',')[1] for line in lines if line.startswith('# of Channels,'))))\n",
    "        scan_rate = float(next(line.split(',')[1] for line in lines if line.startswith('Scan Rate (Hz),')))\n",
    "        num_samples = float(next(line.split(',')[1] for line in lines if line.startswith('Samples/Channel,')))\n",
    "        \n",
    "        stim_duration = float(next(line.split(',')[1] for line in lines if line.startswith('Stimulus duration (ms),')))\n",
    "        stim_interval = float(next(line.split(',')[1] for line in lines if line.startswith('Inter-Stim delay (sec),')))\n",
    "        \n",
    "        emg_amp_gains = []\n",
    "        for line in lines:\n",
    "            if line.startswith('EMG amp gain ch'):\n",
    "                emg_amp_gains.append(int(float(line.split(',')[1])))\n",
    "\n",
    "    # Create a dictionary to store the session data\n",
    "    session_data = {\n",
    "        'session_info': {\n",
    "            'session_name' : dataset,\n",
    "            'num_channels': num_channels,\n",
    "            'scan_rate': int(scan_rate),\n",
    "            'num_samples': int(num_samples),\n",
    "            'stim_duration' : stim_duration,\n",
    "            'stim_interval' : stim_interval,\n",
    "            'emg_amp_gains': emg_amp_gains\n",
    "        },\n",
    "        'recordings': []\n",
    "    }\n",
    "\n",
    "    # Process each recording for stimulus and EMG data\n",
    "    unsaved_data_files = [] # initilizing list for data files that flagged errors.\n",
    "    for recording_file in csv_paths:  # Replace with your list of recording files\n",
    "        with open(recording_file, 'r') as file:\n",
    "            lines = file.readlines() # load CSV lines into memory.\n",
    "            test_session_id = float(next(line.split(',')[1] for line in lines if line.startswith('Session #,')))\n",
    "            \n",
    "            if test_session_id != session_id: # Test if a second session's file is detected\n",
    "                unsaved_data_files.append(recording_file)\n",
    "                continue\n",
    "        \n",
    "            # Extract Stimulus Value (in volts)\n",
    "            stimulus_v = float(next(line.split(',')[1] for line in lines if line.startswith('Stimulus Value (V),')))\n",
    "\n",
    "            # Extract EMG \"Recorded Data (mV)\"\n",
    "            start_index = None\n",
    "            data_lines = []\n",
    "            for i, line in enumerate(lines):\n",
    "                if line.startswith(\"Recorded Data (mV),\"):\n",
    "                    start_index = i + 1\n",
    "                elif start_index is not None:\n",
    "                    data_lines.extend([value.split(',') for value in line.strip().split('\\n')])\n",
    "                    if line.strip() == \"\":\n",
    "                        break\n",
    "\n",
    "            if num_channels != len(data_lines[0]): # test for a mis-match in the cvs number of channels and the declared number of channels.\n",
    "                unsaved_data_files.append(recording_file)\n",
    "                channel_error = True\n",
    "                continue\n",
    "            \n",
    "            # Create a list to store data for each channel\n",
    "            channel_data = [[] for _ in range(num_channels)]\n",
    "\n",
    "            # Populate the channel data\n",
    "            for row in data_lines:\n",
    "                for i, value in enumerate(row):\n",
    "                    channel_data[i].append(float(value))\n",
    "\n",
    "            # Update the session_data with the number of channels and channel_data\n",
    "            session_data['session_info']['num_channels'] = num_channels\n",
    "            session_data['recordings'].append({\n",
    "                'stimulus_v': stimulus_v,\n",
    "                'channel_data': channel_data\n",
    "            })\n",
    "                \n",
    "    #Check for channel recording errors.\n",
    "    if channel_error:\n",
    "        print(f'>! Error: mis-match in declared and recorded channels detected in {dataset}.')\n",
    "        print('\\tData from the following recordings were not saved to the session data file:')\n",
    "        for file in unsaved_data_files:\n",
    "            print(f'\\t\\t{file}')\n",
    "        pass\n",
    "    # Check for unsaved files with errors.\n",
    "    if len(unsaved_data_files) > 0 and not channel_error:\n",
    "        print(f'>! Error: multiple recording sessions detected in {dataset}.')\n",
    "        print('\\tData from the following recordings were not saved to the session data file:')\n",
    "        for file in unsaved_data_files:\n",
    "            print(f'\\t\\t{file}')\n",
    "        pass\n",
    "\n",
    "    # Save the session data to its own pickle file\n",
    "    save_name = dataset + '_data.pickle'\n",
    "    with open(os.path.join(output_path, save_name), 'wb') as pickle_file:\n",
    "        pickle.dump(session_data, pickle_file)\n",
    "\n",
    "    # Return the number of CSV files attempted and successfully processed.\n",
    "    num_csvs = len(csv_paths)\n",
    "    num_csv_success = len(csv_paths) - len(unsaved_data_files)\n",
    "    return num_csvs, num_csv_success"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def pickle_data (data_path, output_path):\n",
    "\n",
    "data_path = DATA_PATH\n",
    "output_path = OUTPUT_PATH\n",
    "\n",
    "# Process \"\\files_to_analyze\" into Pickle files\n",
    "datasets = [dir for dir in os.listdir(data_path) if os.path.isdir(os.path.join(data_path, dir))] # list all datasets in \"files_to_analyze\" folder\n",
    "print(f'Datasets to process: {datasets}')\n",
    "\n",
    "# Create a pickle file for each dataset in files_to_analyze\n",
    "for dataset in datasets:\n",
    "    dataset_path = os.path.join(data_path, dataset)#.replace('\\\\', '/')\n",
    "    \n",
    "    csv_regex = re.compile(r'.*\\.csv$') #regex to match CSV files only.\n",
    "    csv_names = [item for item in os.listdir(dataset_path) if csv_regex.match(item)] #list of CSV filenames in dataset_path.\n",
    "    csv_paths = [os.path.join(dataset_path, csv_name) for csv_name in csv_names]\n",
    "\n",
    "    # check if there are CSVs in this dataset.\n",
    "    if len(csv_paths) <= 0:\n",
    "        print(f'>! Error: no CSV files detected in \"{dataset}.\" Make sure you converted STMs to CSVs.')\n",
    "        continue\n",
    "\n",
    "    # Call function to extract dataset CSVs into a pickle file.\n",
    "    num_csvs, num_csv_success, multipleSessions = pickle_session(dataset, csv_paths, output_path)\n",
    "    if not multipleSessions:\n",
    "        print(f'> {num_csv_success} of {num_csvs} CSVs processed from dataset \"{dataset}\".')\n",
    "        continue\n",
    "    \n",
    "    # If you didn't save the dataset because there were multiple sessions, tries again\n",
    "    if multipleSessions:\n",
    "        num_csvs, num_csv_success = pickle_dataset(dataset, csv_paths, output_path)\n",
    "        print(f'> {num_csv_success} of {num_csvs} CSVs processed from dataset \"{dataset}\".')\n",
    "print('Processing complete.')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "open_ephys",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
